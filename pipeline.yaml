# PIPELINE DEFINITION
# Name: boston-housing-ml-pipeline
# Description: A complete ML pipeline for training and evaluating a model on Boston Housing dataset. The pipeline includes: data extraction from DVC, preprocessing, model training, and evaluation.
# Inputs:
#    dvc_data_path: str [Default: 'data/raw/raw_data.csv']
#    dvc_remote_url: str [Default: '../../dvc-storage']
#    max_depth: int [Default: 0.0]
#    min_samples_split: int [Default: 2.0]
#    n_estimators: int [Default: 100.0]
#    random_state: int [Default: 42.0]
#    scale_features: bool [Default: True]
#    test_size: float [Default: 0.2]
#    threshold_r2: float [Default: 0.5]
components:
  comp-data-extraction:
    executorLabel: exec-data-extraction
    inputDefinitions:
      parameters:
        data_path:
          parameterType: STRING
        dvc_remote_url:
          parameterType: STRING
        output_data_path:
          parameterType: STRING
    outputDefinitions:
      parameters:
        output:
          parameterType: STRING
  comp-data-preprocessing:
    executorLabel: exec-data-preprocessing
    inputDefinitions:
      parameters:
        input_data_path:
          parameterType: STRING
        output_test_path:
          parameterType: STRING
        output_train_path:
          parameterType: STRING
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        scale_features:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        test_size:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      parameters:
        num_features:
          parameterType: NUMBER_INTEGER
        test_size:
          parameterType: NUMBER_INTEGER
        train_size:
          parameterType: NUMBER_INTEGER
  comp-model-evaluation:
    executorLabel: exec-model-evaluation
    inputDefinitions:
      parameters:
        metrics_output_path:
          parameterType: STRING
        model_path:
          parameterType: STRING
        test_data_path:
          parameterType: STRING
        threshold_r2:
          defaultValue: 0.5
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      parameters:
        meets_threshold:
          parameterType: BOOLEAN
        r2_score:
          parameterType: NUMBER_DOUBLE
        rmse:
          parameterType: NUMBER_DOUBLE
  comp-model-training:
    executorLabel: exec-model-training
    inputDefinitions:
      parameters:
        max_depth:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        min_samples_split:
          defaultValue: 2.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_output_path:
          parameterType: STRING
        n_estimators:
          defaultValue: 100.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        train_data_path:
          parameterType: STRING
    outputDefinitions:
      parameters:
        output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-data-extraction:
      container:
        command:
        - sh
        - -c
        - 'set -e

          python3 -m pip install --quiet --no-warn-script-location dvc[s3]>=3.0.0
          pandas>=2.0.0 || pip3 install --quiet --no-warn-script-location dvc[s3]>=3.0.0
          pandas>=2.0.0 || pip install --quiet --no-warn-script-location dvc[s3]>=3.0.0
          pandas>=2.0.0

          "$0" "$@"

          '
        - "def data_extraction(dvc_remote_url, data_path, output_data_path):\n   \
          \ import subprocess\n    import os\n    from pathlib import Path\n    \n\
          \    print(f\"Starting data extraction from DVC...\")\n    print(f\"Remote\
          \ URL: {dvc_remote_url}\")\n    print(f\"Data path: {data_path}\")\n   \
          \ print(f\"Output path: {output_data_path}\")\n    \n    output_dir = os.path.dirname(output_data_path)\n\
          \    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n  \
          \  \n    try:\n        cmd = [\n            'python', '-m', 'dvc', 'get',\n\
          \            '--remote', dvc_remote_url,\n            '.',\n           \
          \ data_path,\n            '-o', output_data_path\n        ]\n        \n\
          \        result = subprocess.run(\n            cmd,\n            capture_output=True,\n\
          \            text=True,\n            check=True\n        )\n        print(f\"\
          Data extraction successful!\")\n    except subprocess.CalledProcessError\
          \ as e:\n        if os.path.exists(data_path):\n            import shutil\n\
          \            shutil.copy(data_path, output_data_path)\n            print(f\"\
          Copied data from local path: {data_path}\")\n        else:\n           \
          \ raise Exception(f\"Failed to extract data: {e.stderr}\")\n    \n    if\
          \ not os.path.exists(output_data_path):\n        raise FileNotFoundError(f\"\
          Output file not created: {output_data_path}\")\n    \n    return output_data_path\n\
          \nimport json\nimport os\n_outputs = {}\n_outputs['Output'] = data_extraction(\n\
          \    dvc_remote_url=json.loads(r'''$0'''),\n    data_path=json.loads(r'''$1'''),\n\
          \    output_data_path=json.loads(r'''$2''')\n)\nos.makedirs('$3', exist_ok=True)\n\
          with open(os.path.join('$3', 'Output'), 'w') as f:\n    f.write(str(_outputs['Output']))\n"
        - '{{$.inputs.parameters[''dvc_remote_url'']}}'
        - '{{$.inputs.parameters[''data_path'']}}'
        - '{{$.inputs.parameters[''output_data_path'']}}'
        - '{{$.outputs.parameters[''output''].output_file}}'
        image: python:3.9
    exec-data-preprocessing:
      container:
        command:
        - sh
        - -c
        - 'set -e

          python3 -m pip install --quiet --no-warn-script-location pandas>=2.0.0 numpy>=1.24.0
          scikit-learn>=1.3.0 || pip3 install --quiet --no-warn-script-location pandas>=2.0.0
          numpy>=1.24.0 scikit-learn>=1.3.0 || pip install --quiet --no-warn-script-location
          pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0

          "$0" "$@"

          '
        - "from typing import NamedTuple\ndef data_preprocessing(input_data_path,\
          \ output_train_path, output_test_path, test_size, random_state, scale_features):\n\
          \    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection\
          \ import train_test_split\n    from sklearn.preprocessing import StandardScaler\n\
          \    from collections import namedtuple\n    import os\n    \n    df = pd.read_csv(input_data_path)\n\
          \    df = df.dropna().drop_duplicates()\n    \n    X = df.iloc[:, :-1]\n\
          \    y = df.iloc[:, -1]\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n\
          \        X, y, test_size=test_size, random_state=random_state\n    )\n \
          \   \n    if scale_features:\n        scaler = StandardScaler()\n      \
          \  X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns,\
          \ index=X_train.index)\n        X_test = pd.DataFrame(scaler.transform(X_test),\
          \ columns=X_test.columns, index=X_test.index)\n    \n    train_df = X_train.copy()\n\
          \    train_df['target'] = y_train.values\n    train_df['split'] = 'train'\n\
          \    \n    test_df = X_test.copy()\n    test_df['target'] = y_test.values\n\
          \    test_df['split'] = 'test'\n    \n    os.makedirs(os.path.dirname(output_train_path),\
          \ exist_ok=True)\n    os.makedirs(os.path.dirname(output_test_path), exist_ok=True)\n\
          \    \n    train_df.to_csv(output_train_path, index=False)\n    test_df.to_csv(output_test_path,\
          \ index=False)\n    \n    output = namedtuple('PreprocessingOutput', ['train_size',\
          \ 'test_size', 'num_features'])\n    return output(train_size=len(train_df),\
          \ test_size=len(test_df), num_features=X_train.shape[1])\n\nimport json\n\
          import os\n_outputs = data_preprocessing(\n    input_data_path=json.loads(r'''$0'''),\n\
          \    output_train_path=json.loads(r'''$1'''),\n    output_test_path=json.loads(r'''$2'''),\n\
          \    test_size=float(json.loads(r'''$3''')),\n    random_state=int(json.loads(r'''$4''')),\n\
          \    scale_features=json.loads(r'''$5''').lower() == 'true'\n)\nos.makedirs('$6',\
          \ exist_ok=True)\nwith open(os.path.join('$6', 'train_size'), 'w') as f:\n\
          \    f.write(str(_outputs.train_size))\nos.makedirs('$7', exist_ok=True)\n\
          with open(os.path.join('$7', 'test_size'), 'w') as f:\n    f.write(str(_outputs.test_size))\n\
          os.makedirs('$8', exist_ok=True)\nwith open(os.path.join('$8', 'num_features'),\
          \ 'w') as f:\n    f.write(str(_outputs.num_features))\n"
        - '{{$.inputs.parameters[''input_data_path'']}}'
        - '{{$.inputs.parameters[''output_train_path'']}}'
        - '{{$.inputs.parameters[''output_test_path'']}}'
        - '{{$.inputs.parameters[''test_size'']}}'
        - '{{$.inputs.parameters[''random_state'']}}'
        - '{{$.inputs.parameters[''scale_features'']}}'
        - '{{$.outputs.parameters[''train_size''].output_file}}'
        - '{{$.outputs.parameters[''test_size''].output_file}}'
        - '{{$.outputs.parameters[''num_features''].output_file}}'
        image: python:3.9
    exec-model-evaluation:
      container:
        command:
        - sh
        - -c
        - 'set -e

          python3 -m pip install --quiet --no-warn-script-location pandas>=2.0.0 numpy>=1.24.0
          scikit-learn>=1.3.0 joblib>=1.3.0 || pip3 install --quiet --no-warn-script-location
          pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0 joblib>=1.3.0 || pip install
          --quiet --no-warn-script-location pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0
          joblib>=1.3.0

          "$0" "$@"

          '
        - "from typing import NamedTuple\ndef model_evaluation(model_path, test_data_path,\
          \ metrics_output_path, threshold_r2):\n    import pandas as pd\n    import\
          \ numpy as np\n    import joblib\n    from sklearn.metrics import mean_squared_error,\
          \ mean_absolute_error, r2_score, explained_variance_score\n    from collections\
          \ import namedtuple\n    import os\n    \n    model = joblib.load(model_path)\n\
          \    test_df = pd.read_csv(test_data_path)\n    \n    X_test = test_df.drop(['target',\
          \ 'split'], axis=1)\n    y_test = test_df['target']\n    \n    y_pred =\
          \ model.predict(X_test)\n    \n    mse = mean_squared_error(y_test, y_pred)\n\
          \    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test, y_pred)\n\
          \    r2 = r2_score(y_test, y_pred)\n    explained_variance = explained_variance_score(y_test,\
          \ y_pred)\n    \n    meets_threshold = r2 >= threshold_r2\n    \n    os.makedirs(os.path.dirname(metrics_output_path),\
          \ exist_ok=True)\n    with open(metrics_output_path, 'w') as f:\n      \
          \  f.write(\"MODEL EVALUATION METRICS\\n\")\n        f.write(f\"R2 Score:\
          \ {r2:.6f}\\n\")\n        f.write(f\"RMSE: {rmse:.6f}\\n\")\n        f.write(f\"\
          MSE: {mse:.6f}\\n\")\n        f.write(f\"MAE: {mae:.6f}\\n\")\n        f.write(f\"\
          Explained Variance: {explained_variance:.6f}\\n\")\n        f.write(f\"\
          Threshold (R2): {threshold_r2:.4f}\\n\")\n        f.write(f\"Meets Threshold:\
          \ {meets_threshold}\\n\")\n    \n    output = namedtuple('EvaluationOutput',\
          \ ['meets_threshold', 'r2_score', 'rmse'])\n    return output(meets_threshold=meets_threshold,\
          \ r2_score=float(r2), rmse=float(rmse))\n\nimport json\nimport os\n_outputs\
          \ = model_evaluation(\n    model_path=json.loads(r'''$0'''),\n    test_data_path=json.loads(r'''$1'''),\n\
          \    metrics_output_path=json.loads(r'''$2'''),\n    threshold_r2=float(json.loads(r'''$3'''))\n\
          )\nos.makedirs('$4', exist_ok=True)\nwith open(os.path.join('$4', 'meets_threshold'),\
          \ 'w') as f:\n    f.write(str(_outputs.meets_threshold).lower())\nos.makedirs('$5',\
          \ exist_ok=True)\nwith open(os.path.join('$5', 'r2_score'), 'w') as f:\n\
          \    f.write(str(_outputs.r2_score))\nos.makedirs('$6', exist_ok=True)\n\
          with open(os.path.join('$6', 'rmse'), 'w') as f:\n    f.write(str(_outputs.rmse))\n"
        - '{{$.inputs.parameters[''model_path'']}}'
        - '{{$.inputs.parameters[''test_data_path'']}}'
        - '{{$.inputs.parameters[''metrics_output_path'']}}'
        - '{{$.inputs.parameters[''threshold_r2'']}}'
        - '{{$.outputs.parameters[''meets_threshold''].output_file}}'
        - '{{$.outputs.parameters[''r2_score''].output_file}}'
        - '{{$.outputs.parameters[''rmse''].output_file}}'
        image: python:3.9
    exec-model-training:
      container:
        command:
        - sh
        - -c
        - 'set -e

          python3 -m pip install --quiet --no-warn-script-location pandas>=2.0.0 numpy>=1.24.0
          scikit-learn>=1.3.0 joblib>=1.3.0 || pip3 install --quiet --no-warn-script-location
          pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0 joblib>=1.3.0 || pip install
          --quiet --no-warn-script-location pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0
          joblib>=1.3.0

          "$0" "$@"

          '
        - "def model_training(train_data_path, model_output_path, n_estimators, max_depth,\
          \ min_samples_split, random_state):\n    import pandas as pd\n    import\
          \ numpy as np\n    import joblib\n    from sklearn.ensemble import RandomForestRegressor\n\
          \    import os\n    \n    train_df = pd.read_csv(train_data_path)\n    X_train\
          \ = train_df.drop(['target', 'split'], axis=1)\n    y_train = train_df['target']\n\
          \    \n    max_depth_param = None if max_depth == 0 else max_depth\n   \
          \ \n    rf_model = RandomForestRegressor(\n        n_estimators=n_estimators,\n\
          \        max_depth=max_depth_param,\n        min_samples_split=min_samples_split,\n\
          \        random_state=random_state,\n        n_jobs=-1,\n        verbose=1\n\
          \    )\n    \n    rf_model.fit(X_train, y_train)\n    \n    os.makedirs(os.path.dirname(model_output_path),\
          \ exist_ok=True)\n    joblib.dump(rf_model, model_output_path)\n    \n \
          \   return model_output_path\n\nimport json\nimport os\n_outputs = {}\n\
          _outputs['Output'] = model_training(\n    train_data_path=json.loads(r'''$0'''),\n\
          \    model_output_path=json.loads(r'''$1'''),\n    n_estimators=int(json.loads(r'''$2''')),\n\
          \    max_depth=int(json.loads(r'''$3''')),\n    min_samples_split=int(json.loads(r'''$4''')),\n\
          \    random_state=int(json.loads(r'''$5'''))\n)\nos.makedirs('$6', exist_ok=True)\n\
          with open(os.path.join('$6', 'Output'), 'w') as f:\n    f.write(str(_outputs['Output']))\n"
        - '{{$.inputs.parameters[''train_data_path'']}}'
        - '{{$.inputs.parameters[''model_output_path'']}}'
        - '{{$.inputs.parameters[''n_estimators'']}}'
        - '{{$.inputs.parameters[''max_depth'']}}'
        - '{{$.inputs.parameters[''min_samples_split'']}}'
        - '{{$.inputs.parameters[''random_state'']}}'
        - '{{$.outputs.parameters[''output''].output_file}}'
        image: python:3.9
pipelineInfo:
  description: 'A complete ML pipeline for training and evaluating a model on Boston
    Housing dataset. The pipeline includes: data extraction from DVC, preprocessing,
    model training, and evaluation.'
  name: boston-housing-ml-pipeline
root:
  dag:
    tasks:
      data-extraction:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-extraction
        inputs:
          parameters:
            data_path:
              componentInputParameter: dvc_data_path
            dvc_remote_url:
              componentInputParameter: dvc_remote_url
            output_data_path:
              runtimeValue:
                constant: data/extracted/raw_data.csv
        taskInfo:
          name: 1. Extract Data from DVC
      data-preprocessing:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-preprocessing
        dependentTasks:
        - data-extraction
        inputs:
          parameters:
            input_data_path:
              taskOutputParameter:
                outputParameterKey: output
                producerTask: data-extraction
            output_test_path:
              runtimeValue:
                constant: data/processed/test.csv
            output_train_path:
              runtimeValue:
                constant: data/processed/train.csv
            random_state:
              componentInputParameter: random_state
            scale_features:
              componentInputParameter: scale_features
            test_size:
              componentInputParameter: test_size
        taskInfo:
          name: 2. Preprocess Data
      model-evaluation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-evaluation
        dependentTasks:
        - model-training
        inputs:
          parameters:
            metrics_output_path:
              runtimeValue:
                constant: metrics/evaluation_metrics.txt
            model_path:
              taskOutputParameter:
                outputParameterKey: output
                producerTask: model-training
            test_data_path:
              runtimeValue:
                constant: data/processed/test.csv
            threshold_r2:
              componentInputParameter: threshold_r2
        taskInfo:
          name: 4. Evaluate Model
      model-training:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-training
        dependentTasks:
        - data-preprocessing
        inputs:
          parameters:
            max_depth:
              componentInputParameter: max_depth
            min_samples_split:
              componentInputParameter: min_samples_split
            model_output_path:
              runtimeValue:
                constant: models/random_forest_model.joblib
            n_estimators:
              componentInputParameter: n_estimators
            random_state:
              componentInputParameter: random_state
            train_data_path:
              runtimeValue:
                constant: data/processed/train.csv
        taskInfo:
          name: 3. Train Random Forest Model
  inputDefinitions:
    parameters:
      dvc_data_path:
        defaultValue: data/raw/raw_data.csv
        description: Path to the data file in DVC repository
        isOptional: true
        parameterType: STRING
      dvc_remote_url:
        defaultValue: ../../dvc-storage
        description: URL or path to DVC remote storage
        isOptional: true
        parameterType: STRING
      max_depth:
        defaultValue: 0.0
        description: 'Maximum depth of trees, 0 = unlimited (default: 0)'
        isOptional: true
        parameterType: NUMBER_INTEGER
      min_samples_split:
        defaultValue: 2.0
        description: 'Minimum samples to split a node (default: 2)'
        isOptional: true
        parameterType: NUMBER_INTEGER
      n_estimators:
        defaultValue: 100.0
        description: 'Number of trees in Random Forest (default: 100)'
        isOptional: true
        parameterType: NUMBER_INTEGER
      random_state:
        defaultValue: 42.0
        description: 'Random state for reproducibility (default: 42)'
        isOptional: true
        parameterType: NUMBER_INTEGER
      scale_features:
        defaultValue: true
        description: 'Whether to scale features using StandardScaler (default: True)'
        isOptional: true
        parameterType: BOOLEAN
      test_size:
        defaultValue: 0.2
        description: 'Proportion of data for testing (default: 0.2)'
        isOptional: true
        parameterType: NUMBER_DOUBLE
      threshold_r2:
        defaultValue: 0.5
        description: 'Minimum R2 score threshold for model acceptance (default: 0.5)'
        isOptional: true
        parameterType: NUMBER_DOUBLE
schemaVersion: 2.1.0
sdkVersion: kfp-2.15.1
